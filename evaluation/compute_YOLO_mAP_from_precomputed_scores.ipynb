{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all we need to compute the mAP and produce some plots\n",
    "# implementing mAP as documented in The PASCALVisual Object Classes (VOC) Challenge\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all computed results \n",
    "# (.pkl format from \"darknet_evaluation_post_inference.py\")\n",
    "\n",
    "input_path = \"D:/WOLO/HPC_trained_models/WOLO_DETECT/RESULTS\"\n",
    "input_files = []\n",
    "\n",
    "for file in os.listdir(input_path):\n",
    "    if file.endswith(\".pkl\"):\n",
    "        input_files.append(os.path.join(input_path,file))\n",
    "        \n",
    "input_files.sort()\n",
    "print(\"Found {} evaluation files.\".format(len(input_files)))\n",
    "for d, dataset in enumerate(input_files):\n",
    "    print(d, dataset)\n",
    "    \n",
    "use_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_files[use_state], 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "- data[0][0]\n",
    "- training_data_&_training_state\n",
    "\n",
    "- - data[0][1][0]\n",
    "- - threshold (for first dataset)\n",
    "  \n",
    "- - - data[0][1][1][0 1  2    3   4   5                  6] \n",
    "- - - dataset_name,   GT, TP, FN, FP, Average Precision, Recall\n",
    "\"\"\"\n",
    "#examples:\n",
    "\n",
    "all_training_states = []\n",
    "\n",
    "for elem in data:\n",
    "    all_training_states.append(elem[0].split(\".\")[0].split(\"_\")[-1])\n",
    "\n",
    "all_training_states.sort()\n",
    "print(all_training_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to retrieve the mean Average Precision (mAP) over 13 confidence thresholds ranging from 0.2 to 0.8, classifying a correct detection centre as being within 10% (of the image width) euclidean distance to a ground truth detection, disregarding multiple detections of the same object as they would be suppressed by non-maxmimum suppresion at run-time. We use this adjusted metric from the original, as the actual intersection over union is secondary to the agreement of centres, as different methods have been used to assign bounding boxes. Synthetically generated bounding boxes are defined as the smallest retangle including all projected 2D keypoints in the rendered images, whereas hand annotated bounding boxes are fixed, square detections, as a custom written centre tracking tool (BlenderMotionExport) was used to semi-automatically produce these datasets.\n",
    "\n",
    "As an example we will plot the precision over recall for these 13 thresholds for the first snapshot of the imported data, and compute the mAP, as in the official [scikit learn implementation](https://github.com/scikit-learn/scikit-learn/blob/baf0ea25d/sklearn/metrics/_ranking.py#L111)\n",
    "\n",
    "(m)AP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:\n",
    "\n",
    "$${AP} = \\sum_n (R_n - R_{n-1}) P_n$$\n",
    "    \n",
    "where `P_n` and `R_n` are the precision and recall at the nth threshold. Using decreasing threshold values, the Recall $R_{n-1}$ at the first threshold is set to 0 as when the threshold is maximal, no detections are returned. Therefore, with no positives returned, the precision $P_{n-1}$ is by definition equal to 1.\n",
    "\n",
    "*This implementation is not interpolated and is different from computing the area under the precision-recall curve with the trapezoidal rule, which uses linear interpolation and can be too optimistic.*\n",
    "\n",
    "**Note:** this implementation is restricted to the binary classification task or multilabel classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_dataset_name(file_name,verbose=False):\n",
    "    \"\"\"\n",
    "    return the name of the dataset wihtout the split extension\n",
    "    \"\"\"\n",
    "    base_name = os.path.basename(file_name)\n",
    "    dataset_name = base_name.split(\"_RESULTS\")[0]\n",
    "    if verbose:\n",
    "        print(dataset_name)\n",
    "    return dataset_name\n",
    "\n",
    "print(\"Found {} evaluation files.\".format(len(input_files)))\n",
    "    \n",
    "all_nets_all_APs = []\n",
    "\n",
    "current_AP_group = []\n",
    "training_datasets = [clean_dataset_name(f) for f in input_files]\n",
    "\n",
    "prev_dataset = clean_dataset_name(input_files[use_state])\n",
    "\n",
    "for use_state in range(len(input_files)):\n",
    "    with open(input_files[use_state], 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    all_training_states = []\n",
    "\n",
    "    for elem in data:\n",
    "        all_training_states.append(elem[0].split(\".\")[0].split(\"_\")[-1])\n",
    "\n",
    "    all_training_states.sort()\n",
    "\n",
    "    final_AP = []\n",
    "    #print(\"\\n\",input_files[use_state])\n",
    "    \n",
    "    for dataset_idx in range(1,15):\n",
    "        all_AP = []\n",
    "\n",
    "        for model in data:\n",
    "            curve_coords = np.zeros([len(model[1:]),2])\n",
    "            AP = 0\n",
    "            R_n = 0\n",
    "            for e, elem in reversed(list(enumerate(model[1:]))):\n",
    "                curve_coords[e] = [elem[dataset_idx][6],elem[dataset_idx][5]]\n",
    "                AP += (elem[dataset_idx][6] - R_n) * elem[dataset_idx][5]\n",
    "                R_n = elem[dataset_idx][6]\n",
    "\n",
    "            #print(\"AP: {}\\n\".format(AP))\n",
    "            all_AP.append([model[1][dataset_idx][0],model[0].split(\"\\\\\")[-2],AP])\n",
    "\n",
    "        all_AP.sort()\n",
    "        final_AP.append(all_AP)\n",
    "        \n",
    "    all_nets_all_APs.append(final_AP)\n",
    "    \n",
    "output_AP = []\n",
    "\n",
    "for model in all_nets_all_APs:\n",
    "    print(\"\\n\\nAP scores for:\", model[0][0][1])\n",
    "    \n",
    "    model_mAP = np.mean([i[0][2] for i in model])\n",
    "    model_stdAP = np.std([i[0][2] for i in model])\n",
    "    categories = [i[0][0][:-6] for i in model]\n",
    "    \n",
    "    print(\"mAP:\", model_mAP, \"+/-\" ,model_stdAP,\"\\n\")\n",
    "    \n",
    "    dataset_APs = []\n",
    "    for dataset in model:\n",
    "        print(\"dataset\", dataset[0][0][:18], \"  AP\", dataset[0][2])\n",
    "        dataset_APs.append(round(dataset[0][2],4))\n",
    "        \n",
    "    dataset_APs.append(round(model_mAP,4))\n",
    "    dataset_APs.append(round(model_stdAP,4))\n",
    "    \n",
    "    output_AP.append(dataset_APs)\n",
    "        \n",
    "categories.append(\"mAP\")\n",
    "categories.append(\"sdtAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_dataframe = pd.DataFrame(output_AP, index = training_datasets, columns=categories)\n",
    "final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_name = \"results_mAP\"\n",
    "\n",
    "final_dataframe.to_csv(os.path.join(input_path,custom_name) + \".csv\")\n",
    "\n",
    "final_dataframe.to_hdf(\n",
    "    os.path.join(input_path,custom_name) + \".h5\",\n",
    "    \"df_with_missing\",\n",
    "    format=\"table\",\n",
    "    mode=\"w\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
