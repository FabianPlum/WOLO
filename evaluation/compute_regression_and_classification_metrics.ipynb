{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WOLO - compute regression and classification metrics\n",
    "This script takes the _test_data_pred_results.csv_ files produced during network fitting and evaluation on test-data as an input and computes the desired output metrics and plots:\n",
    "\n",
    "- MAPE_true\n",
    "- MAPE_ideal\n",
    "- MAPE_class\n",
    "- classification accuracy\n",
    "- confusion matrices\n",
    "- class-wise scores\n",
    "- coefficient of variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "# call the following once to produce resized plots across the notebook\n",
    "plt.show()\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "OUTPUT_LOCATION = \"example_data/\"\n",
    "\n",
    "\"\"\"\n",
    "Set your input file and where the results are to be saved\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "DEFAULT INFERENCE\n",
    "\"\"\"\n",
    "input_folder = \"example_data/5_class\"\n",
    "input_file = input_folder.replace(\"\\\\\",\"/\") +\"/test_data_pred_results.csv\"\n",
    "output_name = input_file.split(\"/\")[-2]\n",
    "\"\"\"\n",
    "DETECTION\n",
    "\"\"\"\n",
    "#input_file = \"D:/WOLO/HPC_trained_models/WOLO_DETECT/RESULTS/DETECT_synth-standard_20_test_data_pred_results.csv\"\n",
    "#output_name = input_file.split(\"/\")[-1].split(\"_test_data_pred_results.csv\")[0]\n",
    "\n",
    "print(output_name)\n",
    "output_txt = open(os.path.join(OUTPUT_LOCATION,output_name + \"---ALL_OUTPUTS.txt\"), \"w\")\n",
    "\n",
    "output_txt.write(\"Running evaluation of inference outputs produced by: \" + output_name + \" ...\\n\")\n",
    "print(\"Beginning writing to output file...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(input_file, \"r\")\n",
    "data = list(csv.reader(file, delimiter=\",\"))\n",
    "file.close()\n",
    "\n",
    "if input_file.split(\"/\")[-1].split(\"_\")[0] == \"DETECT\":\n",
    "    DETECTION = True\n",
    "else:\n",
    "    DETECTION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_class(array, value):\n",
    "    array_np = np.asarray(array)\n",
    "    idx = (np.abs(array_np - value)).argmin()\n",
    "    nearest_class = array_np[idx]\n",
    "    pred_class = array.index(nearest_class)\n",
    "    return pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Retrieving the following info from the input file:\", data[0][1:6])\n",
    "file_names = [row[1] for row in data[1:]]\n",
    "\n",
    "five_class = [0.0013, 0.0030, 0.0068, 0.0154, 0.0351]\n",
    "twenty_class = [0.0010, 0.0012, 0.0015, 0.0019, 0.0023, 0.0028,\n",
    "                0.0034, 0.0042, 0.0052, 0.0064, 0.0078, 0.0096,\n",
    "                0.0118, 0.0145, 0.0179, 0.0219, 0.0270, 0.0331,\n",
    "                0.0407, 0.0500]\n",
    "scaled_20 = [int(x * 10001) for x in twenty_class]\n",
    "\n",
    "if len(data[0]) > 11:\n",
    "    CLASS_LIST = twenty_class\n",
    "elif len(data[0]) == 11:\n",
    "    CLASS_LIST = five_class\n",
    "else: \n",
    "    CLASS_LIST = twenty_class # use classification approach of 20 class list for displaying regressor outputs\n",
    "\n",
    "if len(data[0]) < 6 and not DETECTION: # regressors have fewer lines as the output activations aren't relevant\n",
    "    true_classes = [scaled_20.index(int(x.split(\"/\")[1])) for x in [row[1] for row in data[1:]]]\n",
    "    pred_classes = [find_class(twenty_class, float(x)) for x in [row[3] for row in data[1:]]]\n",
    "    true_weight = [float(x) for x in [row[2] for row in data[1:]]]\n",
    "    pred_weight = [float(x) for x in [row[3] for row in data[1:]]]\n",
    "else:\n",
    "    true_classes = [int(x) for x in [row[2] for row in data[1:]]]\n",
    "    pred_classes = [int(x) for x in [row[3] for row in data[1:]]]\n",
    "    true_weight = [float(x) for x in [row[4] for row in data[1:]]]\n",
    "    pred_weight = [float(x) for x in [row[5] for row in data[1:]]]\n",
    "    \n",
    "if DETECTION:\n",
    "    if len(np.unique(true_classes)) == 5:\n",
    "        CLASS_LIST = five_class\n",
    "    else:\n",
    "        CLASS_LIST = twenty_class\n",
    "    \n",
    "    msg = str(len(np.unique(true_classes))) + \" class detection data found!\"\n",
    "    print(msg)\n",
    "    output_txt.write(msg+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute overall MAPE and accuracy scores\n",
    "across the full test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goAPE(y_true,y_pred,CLASS_LIST=None,gt_v_class=False):\n",
    "    \"\"\"\n",
    "    y_true : gt label vector of lenght n\n",
    "    y_pred : prediction label vector of length n\n",
    "    CLASS_LIST : lookup table of class centres (optional)\n",
    "    return : MAPE, STDAPE\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred), \"Mismatch between input vectors\"\n",
    "    if CLASS_LIST is None:\n",
    "        APE = [np.abs((x[0] - x[1])/x[0]) for x in zip(y_true,y_pred) if x[1] != -1]\n",
    "    elif gt_v_class:\n",
    "        APE = [np.abs((x[0] - CLASS_LIST[x[1]])/x[0]) for x in zip(y_true,y_pred) if x[1] != -1]\n",
    "    else:\n",
    "        APE = [np.abs((CLASS_LIST[x[0]] - CLASS_LIST[x[1]])/CLASS_LIST[x[0]]) for x in zip(y_true,y_pred) if x[1] != -1]\n",
    "    \n",
    "    MAPE = 100 * np.mean(APE)\n",
    "    STDAPE = 100 * np.std(APE)\n",
    "    \n",
    "    return MAPE, STDAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MAPE_true, STDAPE_true = goAPE(y_true=true_weight,\n",
    "                               y_pred=pred_weight)\n",
    "msg = \"MAPE_true  : \" + str(round(MAPE_true,2)) + \"  STDAPE_true : \" + str(round(STDAPE_true,2))\n",
    "output_txt.write(msg+\"\\n\")\n",
    "print(msg)\n",
    "\n",
    "if CLASS_LIST is not None:\n",
    "    MAPE_ideal, STDAPE_ideal = goAPE(y_true=true_weight,\n",
    "                                     y_pred=true_classes,\n",
    "                                     CLASS_LIST=CLASS_LIST,\n",
    "                                     gt_v_class=True)\n",
    "    msg = \"MAPE_ideal : \" + str(round(MAPE_ideal,2)) + \"  STDAPE_ideal : \" + str(round(STDAPE_ideal,2))\n",
    "    output_txt.write(msg+\"\\n\")\n",
    "    print(msg)\n",
    "\n",
    "    MAPE_class, STDAPE_class = goAPE(y_true=true_classes,\n",
    "                                     y_pred=pred_classes,\n",
    "                                     CLASS_LIST=CLASS_LIST)\n",
    "    msg = \"MAPE_class : \" + str(round(MAPE_class,2)) + \"  STDAPE_class : \" + str(round(STDAPE_class,2))\n",
    "    output_txt.write(msg+\"\\n\")\n",
    "    print(msg)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(y_true=true_classes,\n",
    "                                      y_pred=pred_classes)\n",
    "    \n",
    "    msg = \"Classification accuracy : \" + str(round(accuracy,4))\n",
    "    output_txt.write(msg+\"\\n\\n\")\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DETECTION:\n",
    "    true_classes_cleaned = [x[0] for x in zip(true_classes,pred_classes) if x[1] != -1]\n",
    "    pred_classes_cleaned = [x for x in pred_classes if x != -1]\n",
    "    file_names_cleaned = [x[0] for x in zip(file_names,pred_classes) if x[1] != -1]\n",
    "    true_weight_cleaned = [x[0] for x in zip(true_weight,pred_classes) if x[1] != -1]\n",
    "    pred_weight_cleaned = [x[0] for x in zip(pred_weight,pred_classes) if x[1] != -1]\n",
    "    \n",
    "    true_classes = true_classes_cleaned\n",
    "    pred_classes = pred_classes_cleaned\n",
    "    file_names = file_names_cleaned\n",
    "    true_weight = true_weight_cleaned\n",
    "    pred_weight = pred_weight_cleaned\n",
    "\n",
    "y_actu = pd.Series([CLASS_LIST[x] for x in true_classes], name='True class')\n",
    "y_pred = pd.Series([CLASS_LIST[x] for x in pred_classes], name='Predicted class')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "df_conf_norm = df_confusion.div(df_confusion.sum(axis=1), axis=\"index\")\n",
    "\n",
    "df_conf_norm.to_csv(os.path.join(OUTPUT_LOCATION,output_name + \"---Confusion_matrix.csv\"))\n",
    "\n",
    "df_conf_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(true_classes, pred_classes, normalize=\"true\")\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, \n",
    "                                            display_labels = CLASS_LIST)\n",
    "\n",
    "cm_display.plot(cmap=plt.cm.gray_r, \n",
    "                include_values=False, #set to true to display individual values\n",
    "                xticks_rotation=45)\n",
    "\n",
    "# update colour scale to enforce displaying normalised values between 0 and 1\n",
    "for im in plt.gca().get_images():\n",
    "    im.set_clim(vmin=0,vmax=1)  \n",
    "    \n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.savefig(os.path.join(OUTPUT_LOCATION,output_name + \"---Confusion_matrix.svg\"), dpi='figure', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute class wise scores \n",
    "and **prediction stability** in terms of the **average coefficient of variation** across continuous samples containing the same individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comb = zip(file_names,true_classes,pred_classes,true_weight,pred_weight)\n",
    "\n",
    "prev_class_temp = true_classes[0]\n",
    "ind_list = {}\n",
    "\n",
    "class_wise_scores = []\n",
    "\n",
    "class_wise_elements_gt_cl = []\n",
    "class_wise_elements_p_cl = []\n",
    "class_wise_elements_gt = []\n",
    "class_wise_elements_p = []\n",
    "\n",
    "output_txt.write(\"\\n- - - - - - - - - - - - - - - - - - - - - - - - - - - -\\n\")\n",
    "output_txt.write(\"\\n Class-wise scores: \\n\")\n",
    "\n",
    "for f,gt_cl,p_cl,gt,p in data_comb:    \n",
    "    file_components = f.split(\"/\")\n",
    "    class_temp = gt_cl\n",
    "    # cut away the frame number so individuals have consistent names\n",
    "    vid = \"_\".join(file_components[2].split(\"_\")[0:-2]) + \"_\" + file_components[2].split(\"_\")[-1]\n",
    "    \n",
    "    if vid not in ind_list:\n",
    "        ind_list[vid] = []\n",
    "    \n",
    "    \"\"\"\n",
    "    # use the following instead of line below, when extracting error stability instead of prediction stability\n",
    "    if CLASS_LIST is not None:\n",
    "        APE_temp = np.abs((CLASS_LIST[gt_cl] - CLASS_LIST[p_cl])/CLASS_LIST[gt_cl])\n",
    "    else:\n",
    "        APE_temp = np.abs((gt - p)/gt)\n",
    "        \n",
    "    ind_list[vid].append([gt, APE_temp])\n",
    "    \"\"\"    \n",
    "    \n",
    "    ind_list[vid].append([gt,p])\n",
    "    \n",
    "    \n",
    "    \n",
    "    if class_temp != prev_class_temp or f == file_names[-1]:\n",
    "        if f == file_names[-1]:\n",
    "            # in case this is the last element, add the final line before computing scores\n",
    "            class_wise_elements_gt_cl.append(gt_cl)\n",
    "            class_wise_elements_p_cl.append(p_cl)\n",
    "            class_wise_elements_gt.append(gt)\n",
    "            class_wise_elements_p.append(p)\n",
    "        \n",
    "        msg = \"\\nCLASS : \" + str(CLASS_LIST[prev_class_temp])\n",
    "        output_txt.write(msg+\"\\n\")\n",
    "        print(msg)\n",
    "\n",
    "        MAPE_true, STDAPE_true = goAPE(y_true=class_wise_elements_gt,\n",
    "                                       y_pred=class_wise_elements_p)\n",
    "        msg = \"MAPE_true  : \" + str(round(MAPE_true,2)) + \"  STDAPE_true : \" + str(round(STDAPE_true,2))\n",
    "        output_txt.write(msg+\"\\n\")\n",
    "        print(msg)\n",
    "\n",
    "        MAPE_ideal, STDAPE_ideal = goAPE(y_true=class_wise_elements_gt,\n",
    "                                         y_pred=class_wise_elements_gt_cl,\n",
    "                                         CLASS_LIST=CLASS_LIST,\n",
    "                                         gt_v_class=True)\n",
    "        msg = \"MAPE_ideal : \" + str(round(MAPE_ideal,2)) + \"  STDAPE_ideal : \" + str(round(STDAPE_ideal,2))\n",
    "        output_txt.write(msg+\"\\n\")\n",
    "        print(msg)\n",
    "\n",
    "        MAPE_class, STDAPE_class = goAPE(y_true=class_wise_elements_gt_cl,\n",
    "                                         y_pred=class_wise_elements_p_cl,\n",
    "                                         CLASS_LIST=CLASS_LIST)\n",
    "        msg = \"MAPE_class : \" + str(round(MAPE_class,2)) + \"  STDAPE_class : \" + str(round(STDAPE_class,2))\n",
    "        output_txt.write(msg+\"\\n\")\n",
    "        print(msg)\n",
    "\n",
    "        accuracy = metrics.accuracy_score(y_true=class_wise_elements_gt_cl,\n",
    "                                          y_pred=class_wise_elements_p_cl)       \n",
    "        msg = \"Classification accuracy : \" + str(round(accuracy,4))\n",
    "        output_txt.write(msg+\"\\n\")\n",
    "        print(msg)\n",
    "\n",
    "        class_wise_scores.append([prev_class_temp,\n",
    "                                  MAPE_true, STDAPE_true,\n",
    "                                  MAPE_ideal, STDAPE_ideal,\n",
    "                                  MAPE_class, STDAPE_class,\n",
    "                                  accuracy])\n",
    "        \n",
    "        prev_class_temp = class_temp\n",
    "        class_wise_elements_gt_cl = []\n",
    "        class_wise_elements_p_cl = []\n",
    "        class_wise_elements_gt = []\n",
    "        class_wise_elements_p = []\n",
    "    \n",
    "    class_wise_elements_gt_cl.append(gt_cl)\n",
    "    class_wise_elements_p_cl.append(p_cl)\n",
    "    class_wise_elements_gt.append(gt)\n",
    "    class_wise_elements_p.append(p)\n",
    "    \n",
    "class_wise_scores = np.array(class_wise_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_var_ind = []\n",
    "\n",
    "for key, value in ind_list.items() :\n",
    "    coeff_var_ind.append(np.std([i[1] for i in value])/np.mean([i[1] for i in value]))\n",
    " \n",
    "msg = \"Average coefficient of variation across repeated predictions: \" + str(round(np.mean(coeff_var_ind),4))\n",
    "print(msg)\n",
    "output_txt.write(\"\\n- - - - - - - - - - - - - - - - - - - - - - - - - - - -\\n\")\n",
    "output_txt.write(\"\\n\" + msg+\"\\n\")\n",
    "output_txt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class wise score visualisation\n",
    "Finally, plot the resulting class-wise MAPE (comparing prediction to ground truth, regardless of inference method) and class-wise accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [6, 4]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(np.arange(len(CLASS_LIST)), class_wise_scores[:,1], \n",
    "       #yerr=class_wise_scores[:,2], \n",
    "       align='center', \n",
    "       alpha=0.5, \n",
    "       ecolor='black', capsize=10)\n",
    "\n",
    "ax.set_ylabel('MAPE')\n",
    "ax.set_xticks(np.arange(len(CLASS_LIST)))\n",
    "ax.set_xticklabels(CLASS_LIST, rotation=45)\n",
    "ax.set_title('class-wise MAPE')\n",
    "ax.yaxis.grid(True)\n",
    "#ax.set_yscale('log')\n",
    "ax.set_ylim(1,500)\n",
    "\n",
    "# Save the figure and show\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_LOCATION,output_name + \"---class-wise_MAPE.svg\"), dpi='figure', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [6, 4]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(np.arange(len(CLASS_LIST)), class_wise_scores[:,-1], \n",
    "       #yerr=class_wise_scores[:,2], \n",
    "       align='center', \n",
    "       alpha=0.5, \n",
    "       ecolor='black', capsize=10)\n",
    "\n",
    "ax.set_ylabel('MAPE')\n",
    "ax.set_xticks(np.arange(len(CLASS_LIST)))\n",
    "ax.set_xticklabels(CLASS_LIST, rotation=45)\n",
    "ax.set_title('class-wise accuracy')\n",
    "ax.yaxis.grid(True)\n",
    "ax.set_ylim(0,1)\n",
    "\n",
    "# Save the figure and show\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_LOCATION,output_name + \"---class-wise_accuracy.svg\"), dpi='figure', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction vs ground truth scatter plot (log log scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_v_pred_xy = []\n",
    "for key, value in ind_list.items():\n",
    "    gt_v_pred_xy.append([value[0][0], np.mean([i[1] for i in value])])\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [6, 6]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter([i[0] for i in gt_v_pred_xy], \n",
    "           [i[1] for i in gt_v_pred_xy], \n",
    "           marker=None, cmap=None, \n",
    "           vmin=0.0005, vmax=0.05, \n",
    "           alpha=0.1)\n",
    "\n",
    "\"\"\"\n",
    "ax.set_xticks(CLASS_LIST)\n",
    "ax.set_xticklabels(CLASS_LIST, rotation=45)\n",
    "\n",
    "ax.set_yticks(CLASS_LIST)\n",
    "ax.set_yticklabels(CLASS_LIST)\n",
    "\"\"\"\n",
    "\n",
    "ax.set_ylabel('predicted weight [g]')\n",
    "ax.set_xlabel('ground truth weight [g]')\n",
    "ax.set_title('gt vs predicted weight')\n",
    "ax.yaxis.grid(True)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylim(0.001,0.05)    \n",
    "ax.set_xlim(0.001,0.05)  \n",
    "\n",
    "# Save the figure and show\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_LOCATION,output_name + \"---gt_vs_predicted_weight.svg\"), \n",
    "            dpi='figure', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
