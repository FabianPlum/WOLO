{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136b1bf8",
   "metadata": {},
   "source": [
    "SO \n",
    "Here is how I'm gonna do this\n",
    "as a first try, without any preprocessing, simply combine all the data into a single\n",
    "dataframe, regardless of threshold, just to nail down how the data needs to be structured\n",
    "\n",
    "one by one, read in the csv files, creating one line in the data frame for every 30 frames\n",
    "then write the identity in terms of video, bramble-or-rose, startframe, id, extracted body-length, as the \n",
    "first 5 elements in the row. The 6th entry is then the size class, so we can use that as a\n",
    "label. without any smoothing, angles, or speeds, the shape of the data-frame will then be\n",
    "\n",
    "(n, 30 * num_key_points + 6)\n",
    "\n",
    "to run dimensionality reduction on that data, simply repeat the setup above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7362eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981a0ea-a6b7-4478-83b8-ac5d45cebec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FILE\n",
    "df_ant_poses_1 = pd.read_pickle(\"2019-07-23_bramble_right_POSE_FEATURE_DICT.pkl\")  \n",
    "df_ant_poses_2 = pd.read_pickle(\"2019-08-03_bramble-right_POSE_FEATURE_DICT.pkl\")  \n",
    "df_ant_poses_3 = pd.read_pickle(\"2019-07-23_rose_left_POSE_FEATURE_DICT.pkl\")  \n",
    "\n",
    "# stack the two DataFrames\n",
    "df_ant_poses = pd.concat([df_ant_poses_1, df_ant_poses_2, df_ant_poses_3], ignore_index=True, axis=0)\n",
    "\n",
    "features = df_ant_poses.loc[:, 'raw_pose_0':] # use all entries and exclude labels\n",
    "labels =  df_ant_poses.loc[:, :\"size_class\"]\n",
    "\n",
    "norm_features = (features-features.min())/(features.max()-features.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456e71ea-f697-4cf8-8666-85f31c7fbf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"extracted feature vector contains\",norm_features.shape[0],\"instances and\", norm_features.shape[1], \"features\")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41cdc4-bf54-4615-aa80-83ca99205e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a test, run a 2D tSNE\n",
    "tsne = TSNE(n_components=2, random_state=0, init=\"pca\", learning_rate=\"auto\", perplexity=50)\n",
    "projections = tsne.fit_transform(norm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a700d23-51f0-464b-8a9c-e4fd9f8a6e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now take the produced embedding and plot with different colour overlays to group by class, speed, or material\n",
    "fig_size = px.scatter(\n",
    "    projections, x=0, y=1,\n",
    "    color=df_ant_poses.size_class, labels={'color': 'size_class'},\n",
    "    width=600, height=500) \n",
    "\n",
    "fig_size.write_image(\"tSNE_fig_size.svg\")\n",
    "\n",
    "fig_speed = px.scatter(\n",
    "    projections, x=0, y=1,\n",
    "    color=df_ant_poses.speed, labels={'color': 'speed'},\n",
    "    width=600, height=500) \n",
    "\n",
    "fig_speed.write_image(\"tSNE_fig_speed.svg\")\n",
    "\n",
    "fig_material = px.scatter(\n",
    "    projections, x=0, y=1,\n",
    "    color=df_ant_poses.material, labels={'color': 'material'},\n",
    "    width=600, height=500) \n",
    "\n",
    "fig_material.write_image(\"tSNE_fig_material.svg\")\n",
    "\n",
    "fig_size.show()\n",
    "fig_speed.show()\n",
    "fig_material.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef6f14-6e06-499c-a32e-7e6564698010",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_2d = UMAP(n_components=2, init='random')#, random_state=0)\n",
    "umap_3d = UMAP(n_components=3, init='random')#, random_state=0)\n",
    "\n",
    "proj_2d = umap_2d.fit_transform(norm_features)\n",
    "proj_3d = umap_3d.fit_transform(norm_features)\n",
    "\n",
    "fig_size_2d = px.scatter(\n",
    "    proj_2d, x=0, y=1,\n",
    "    color=df_ant_poses.size_class, labels={'color': 'size_class'},\n",
    "    width=600, height=500\n",
    ")\n",
    "\n",
    "fig_size_3d = px.scatter_3d(\n",
    "    proj_3d, x=0, y=1, z=2,\n",
    "    color=df_ant_poses.size_class, labels={'color': 'size_class'},\n",
    "    width=600, height=500\n",
    ")\n",
    "fig_size_3d.update_traces(marker_size=5)\n",
    "\n",
    "\n",
    "fig_speed_2d = px.scatter(\n",
    "    proj_2d, x=0, y=1,\n",
    "    color=df_ant_poses.speed, labels={'color': 'speed'},\n",
    "    width=600, height=500\n",
    ")\n",
    "\n",
    "fig_speed_3d = px.scatter_3d(\n",
    "    proj_3d, x=0, y=1, z=2,\n",
    "    color=df_ant_poses.speed, labels={'color': 'speed'},\n",
    "    width=600, height=500\n",
    ")\n",
    "fig_speed_3d.update_traces(marker_size=5)\n",
    "\n",
    "fig_size_2d.show()\n",
    "fig_size_3d.show()\n",
    "\n",
    "fig_speed_2d.show()\n",
    "fig_speed_3d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d8fb3-23ec-4cdb-aa03-cbbc77f705d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
